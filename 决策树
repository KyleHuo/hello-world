# 决策树算法梳理

## 信息论基础（熵 联合熵 条件熵 信息增益 基尼不纯度） 

## 决策树的不同分类算法（ID3算法、C4.5、CART分类树）的原理及应用场景

## 回归树原理

## 决策树防止过拟合手段

## 模型评估

## sklearn参数详解，Python绘制决策树

# 信息论基础

## 熵

在决策树算法中，熵是一个非常非常重要的概念。

一件事发生的概率越小，我们说它所蕴含的信息量越大。

比如：我们听女人能怀孕不奇怪，如果某天听到哪个男人怀孕了，我们就会觉得emmm…信息量很大了。

所以我们这样衡量信息量：

![img](https://pic3.zhimg.com/80/v2-656b2d5d32d28159e93ba1dd5a4a2776_hd.jpg)

其中，P(y)是事件发生的概率。

信息熵就是所有可能发生的事件的信息量的期望：

![img](https://pic4.zhimg.com/80/v2-0167e4c67f9adae4ca6388aab25f4a63_hd.jpg)

表达了Y事件发生的不确定度。

## 条件熵

条件熵：表示在X给定条件下，Y的条件概率分布的熵对X的数学期望。其数学推导如下：

![img](https://pic3.zhimg.com/80/v2-1c8b4f00b3b86a6bbd90369ab374bec6_hd.jpg)

条件熵H（Y|X）表示在已知随机变量X的条件下随机变量Y的不确定性。注意一下，条件熵中X也是一个变量，意思是在一个变量X的条件下（变量X的每个值都会取到），另一个变量Y的熵对X的期望。

所以我们得到条件熵的计算公式：

![img](https://pic4.zhimg.com/80/v2-48772f8f87597e8a4608ae548914677f_hd.jpg)



＃　联合熵

![img](https://gss0.bdstatic.com/94o3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D269/sign=eb3bf749dd58ccbf1fbcb23c20dabcd4/fd039245d688d43fdf71cf09711ed21b0ff43b74.jpg)

其中ｘ和ｙ是Ｘ和Ｙ的特定值，相应地， ![img](https://gss0.bdstatic.com/94o3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D45/sign=474f7d14a5ec8a13101a56e5f6038062/4ec2d5628535e5ddf772fc247ac6a7efcf1b62ea.jpg) 是这些值一起出现的联合概率, 若

 ![img](https://gss1.bdstatic.com/-vo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D73/sign=b891f901ddc8a786ba2a480d66091009/b21c8701a18b87d6baf1f2e40b0828381e30fd9e.jpg) 则 ![img](https://gss1.bdstatic.com/9vo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D133/sign=4d29e1db0de939015202893d48ee54f9/b2de9c82d158ccbf5d71ecc115d8bc3eb035417f.jpg) 定义为0。

## 信息增益

当我们用另一个变量X对原变量Y分类后，原变量Y的不确定性就会减小了(即熵值减小)。而熵就是不确定性，不确定程度减少了多少其实就是信息增益。这就是信息增益的由来，所以信息增益定义如下：

![img](https://pic2.zhimg.com/80/v2-dc28f78bbb5d66e583143e1be2d69d11_hd.jpg)

＃＃基尼不纯度

将来自集合中的某种结果随机应用于集合中某一数据项的预期误差率。

![img](http://upload.wikimedia.org/wikipedia/en/math/d/8/6/d863d154c9919b078caef57d463a67bd.png)

# 决策树算法

# 算法简介

决策树算法是一类常见的分类和回归算法，顾名思义，决策树是基于树的结构来进行决策的。

以二分类为例，我们希望从给定训练集中学得一个模型来对新的样例进行分类。

**举个例子**

有一个划分是不是鸟类的数据集合，如下：

![img](https://pic3.zhimg.com/80/v2-9f881f92ea00efc4f8b0ec028f09849a_hd.jpg)

这时候我们建立这样一颗决策树：

![img](https://pic2.zhimg.com/80/v2-4a601bdc74abb553c0873fbd61597035_hd.jpg)

当我们有了一组新的数据时，我们就可以根据这个决策树判断出是不是鸟类。创建决策树的伪代码如下：

![img](https://pic4.zhimg.com/80/v2-c226901dc50538bd40410e7aae938f47_hd.jpg)

生成决策树是一个递归的过程，在决策树算法中，当出现下列三种情况时，导致递归返回：

(1)当前节点包含的样本属于同一种类，无需划分；

(2)当前属性集合为空，或者所有样本在所有属性上取值相同，无法划分；

(3)当前节点包含的样本集合为空，无法划分。



## 属性选择

在决策树算法中，最重要的就是划分属性的选择，即我们选择哪一个属性来进行划分。三种划分属性的主要算法是：ID3、C4.5以及CART。

### ID3算法

ID3算法所采用的度量标准就是我们前面所提到的“信息增益”。当属性a的信息增益最大时，则意味着用a属性划分，其所获得的“纯度”提升最大。我们所要做的，就是找到信息增益最大的属性。由于前面已经强调了信息增益的概念，这里不再赘述。

### C4.5算法

实际上，信息增益准则对于可取值数目较多的属性会有所偏好，为了减少这种偏好可能带来的不利影响，C4.5决策树算法不直接使用信息增益，而是使用“信息增益率”来选择最优划分属性，信息增益率定义为：

![img](https://pic1.zhimg.com/80/v2-28e631b8a8ffeaad5a3f449ba5101008_hd.jpg)

其中，分子为信息增益，分母为属性X的熵。

需要注意的是，增益率准则对可取值数目较少的属性有所偏好。

所以一般这样选取划分属性：**先从候选属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。

## CART算法

ID3算法和C4.5算法主要存在三个问题：

(1)每次选取最佳特征来分割数据，并按照该特征的所有取值来进行划分。也就是说，如果一个特征有4种取值，那么数据就将被切成4份，一旦特征被切分后，该特征就不会再起作用，有观点认为这种切分方式过于迅速。

(2)它们不能处理连续型特征。只有事先将连续型特征转换为离散型，才能在上述算法中使用。

(3)会产生过拟合问题。

为了解决上述(1)、(2)问题，产生了CART算法，它主要的衡量指标是基尼系数。为了解决问题(3)，主要采用剪枝技术和随机森林算法，这部分内容，下一次再详细讲述。

上述就是决策树算法的原理部分，下面展示完整代码和注释。代码中主要采用的是ID3算法。



# 回归树原理

如果预测某个连续变量的大小，最简单的模型之一就是用平均值。比如同事的平均年龄是 28 岁，那么新来了一批同事，在不知道这些同事的任何信息的情况下，直觉上用平均值 28 来预测是比较准确的，至少比 0 岁或者 100 岁要靠谱一些。我们不妨证明一下我们的直觉：

1. 定义损失函数 L，其中 y_hat 是对 y 预测值，使用 MSE 来评估损失：
   ![ L = -Largefrac{1}{2}normalsizesum_{i=0}^m(y_i-hat{y}) ^ 2](https://www.zhihu.com/equation?tex=+L+%3D+-%5CLarge%5Cfrac%7B1%7D%7B2%7D%5Cnormalsize%5Csum_%7Bi%3D0%7D%5Em%28y_i-%5Chat%7By%7D%29+%5E+2)
2. 对 y_hat 求导:
   ![ large frac{mathrm{d}L}{mathrm{d}hat{y}} normalsize = sum_{i=0}^m(y_i-hat{y}) = sum_{i=0}^my_i - sum_{i=0}^mhat{y} = sum_{i=0}^my_i - m*hat{y} ](https://www.zhihu.com/equation?tex=+%5Clarge+%5Cfrac%7B%5Cmathrm%7Bd%7DL%7D%7B%5Cmathrm%7Bd%7D%5Chat%7By%7D%7D+%5Cnormalsize+%3D+%5Csum_%7Bi%3D0%7D%5Em%28y_i-%5Chat%7By%7D%29+%3D+%5Csum_%7Bi%3D0%7D%5Emy_i+-+%5Csum_%7Bi%3D0%7D%5Em%5Chat%7By%7D+%3D+%5Csum_%7Bi%3D0%7D%5Emy_i+-+m%2A%5Chat%7By%7D+)
3. 令导数等于 0，最小化 MSE，则:
   ![ sum_{i=0}^my_i - m*hat{y} = 0](https://www.zhihu.com/equation?tex=+%5Csum_%7Bi%3D0%7D%5Emy_i+-+m%2A%5Chat%7By%7D+%3D+0)
4. 所以，
   ![ hat{y} = Largefrac{1}{m}normalsizesum_{i=0}^my_i ](https://www.zhihu.com/equation?tex=+%5Chat%7By%7D+%3D+%5CLarge%5Cfrac%7B1%7D%7Bm%7D%5Cnormalsize%5Csum_%7Bi%3D0%7D%5Emy_i+)
5. 结论，如果要用一个常量来预测 y，用 y 的均值是一个最佳的选择



# 决策树防止过拟合手段

决策树剪枝



# 模型评估

保持方法，Holdout：原数据集分为两个不相交子集，一个训练集用于归纳分类模型，一个检验集用以评估模型性能。

随机二次抽样：用多次重复的保持方法对分类器性能进行评估。

交叉验证，cross-validation：将数据集分为K个子集，第i个集合用作检验集，剩下k-1个用作训练集，交叉k次。当K=2时称为二折交叉验证。当K=N，即子集数量等于记录数量，每个子集只有一个记录时，称为留一法（leave-one-out）。

自助法，bootstrap：训练集记录在训练时有放回得使用。



# sklearn

```text
sklearn.tree.DecisionTreeClassifier        (criterion='gini', splitter='best', max_depth=None, min_samples_split=2, 
        min_samples_leaf=1,min_weight_fraction_leaf=0.0, max_features=None, 
        random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, 
        min_impurity_split=None, class_weight=None, presort=False)
```

`criterion`:特征选择的标准，有信息增益和基尼系数两种，使用信息增益的是ID3和C4.5算法（使用信息增益比），使用基尼系数的CART算法，默认是gini系数。

`splitter`:特征切分点选择标准，决策树是递归地选择最优切分点，spliter是用来指明在哪个集合上来递归，有“best”和“random”两种参数可以选择，best表示在所有特征上递归，适用于数据集较小的时候，random表示随机选择一部分特征进行递归，适用于数据集较大的时候。

`max_depth`:决策树最大深度，决策树模型先对所有数据集进行切分，再在子数据集上继续循环这个切分过程，max_depth可以理解成用来限制这个循环次数。

`min_samples_split`:子数据集再切分需要的最小样本量，默认是2，如果子数据样本量小于2时，则不再进行下一步切分。如果数据量较小，使用默认值就可，如果数据量较大，为降低计算量，应该把这个值增大，即限制子数据集的切分次数。

`min_samples_leaf`:叶节点（子数据集）最小样本数，如果子数据集中的样本数小于这个值，那么该叶节点和其兄弟节点都会被剪枝（去掉），该值默认为1。

`min_weight_fraction_leaf`:在叶节点处的所有输入样本权重总和的最小加权分数，如果不输入则表示所有的叶节点的权重是一致的。

`max_features`:特征切分时考虑的最大特征数量，默认是对所有特征进行切分，也可以传入int类型的值，表示具体的特征个数；也可以是浮点数，则表示特征个数的百分比；还可以是sqrt,表示总特征数的平方根；也可以是log2，表示总特征数的log个特征。

`random_state`:随机种子的设置，与LR中参数一致。

`max_leaf_nodes`:最大叶节点个数，即数据集切分成子数据集的最大个数。

`min_impurity_decrease`:切分点不纯度最小减少程度，如果某个结点的不纯度减少小于这个值，那么该切分点就会被移除。

`min_impurity_split`:切分点最小不纯度，用来限制数据集的继续切分（决策树的生成），如果某个节点的不纯度（可以理解为分类错误率）小于这个阈值，那么该点的数据将不再进行切分。

`class_weight`:权重设置，主要是用于处理不平衡样本，与LR模型中的参数一致，可以自定义类别权重，也可以直接使用balanced参数值进行不平衡样本处理。

`presort`:是否进行预排序，默认是False，所谓预排序就是提前对特征进行排序，我们知道，决策树分割数据集的依据是，优先按照信息增益/基尼系数大的特征来进行分割的，涉及的大小就需要比较，如果不进行预排序，则会在每次分割的时候需要重新把所有特征进行计算比较一次，如果进行了预排序以后，则每次分割的时候，只需要拿排名靠前的特征就可以了。
